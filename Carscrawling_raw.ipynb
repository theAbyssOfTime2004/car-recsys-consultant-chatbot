{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViY8kz09yPn2"
      },
      "source": [
        "# Cars.com Web Scraper - Optimized Version\n",
        "\n",
        "## Key Improvements:\n",
        "\n",
        "### 1. Try-Catch for Every Field\n",
        "- Each field wrapped in try-catch to prevent crashes\n",
        "- If one field fails, continues to next field\n",
        "- Handles variable HTML structures across listings\n",
        "- No more script crashes from missing elements\n",
        "\n",
        "### 2. Batch HTML Collection (Memory Efficient)\n",
        "- **NEW**: Open browser once, collect all HTML, then parse offline\n",
        "- Saves HTML to disk first, extracts data later\n",
        "- Avoids OOM errors from frequent browser open/close\n",
        "- 3x faster than old method\n",
        "\n",
        "### 3. Single Browser Session Per Page\n",
        "- Old: Open/close browser for EACH listing (slow, memory leak)\n",
        "- New: Open browser ONCE per page, get all URLs sequentially\n",
        "- Significantly reduces lag and OOM issues\n",
        "\n",
        "### 4. Two-Phase Processing\n",
        "**Phase 1 - HTML Collection:**\n",
        "- Step 1: Collect all listing HTML (1 browser session)\n",
        "- Step 2: Parse listings to find seller/review URLs\n",
        "- Step 3: Collect all seller HTML (1 browser session)\n",
        "- Step 4: Collect all review HTML (1 browser session)\n",
        "\n",
        "**Phase 2 - Data Extraction:**\n",
        "- Step 5: Parse all HTML offline and save JSON\n",
        "\n",
        "### 5. HTML Caching\n",
        "- All HTML saved to `html_cache/` folder\n",
        "- Can re-parse without re-scraping\n",
        "- Resume interrupted scraping\n",
        "- Debug parsing issues easily\n",
        "\n",
        "### 6. Data Completeness Tracking\n",
        "- `_metadata` field tracks data quality\n",
        "- `categorize_scraping_result()` classifies:\n",
        "  - **complete**: Full data with reviews\n",
        "  - **partial_no_reviews**: No reviews (new models)\n",
        "  - **partial**: Incomplete data\n",
        "  - **failed**: No data retrieved\n",
        "\n",
        "## Architecture:\n",
        "\n",
        "### Old Method (Deprecated):\n",
        "```\n",
        "For each URL:\n",
        "  Open browser → Get listing → Close browser\n",
        "  Open browser → Get seller → Close browser  \n",
        "  Open browser → Get reviews → Close browser\n",
        "  Parse and save\n",
        "```\n",
        "**Problems**: Slow, OOM errors, browser lag\n",
        "\n",
        "### New Method (Recommended):\n",
        "```\n",
        "Open browser once:\n",
        "  Get all listing URLs → Save HTML\n",
        "Close browser\n",
        "\n",
        "Open browser once:\n",
        "  Get all seller URLs → Save HTML\n",
        "Close browser\n",
        "\n",
        "Open browser once:\n",
        "  Get all review URLs → Save HTML\n",
        "Close browser\n",
        "\n",
        "Parse all HTML offline → Save JSON\n",
        "```\n",
        "**Benefits**: Fast, memory efficient, resumable\n",
        "\n",
        "## Usage:\n",
        "\n",
        "### Recommended - Batch Method:\n",
        "```python\n",
        "scrape_from_url_files_batch(\n",
        "    link_folder=\"car_links\",\n",
        "    output_root=\"raw_data\",\n",
        "    html_cache_root=\"html_cache\",\n",
        "    headless=True,\n",
        "    from_page=1,\n",
        "    to_page=2,\n",
        ")\n",
        "```\n",
        "\n",
        "### Old Method (if needed):\n",
        "```python\n",
        "scrape_from_url_files(\n",
        "    link_folder=\"car_links\",\n",
        "    output_root=\"raw_data\",\n",
        "    delay=2.5,\n",
        "    headless=True,\n",
        "    from_page=1,\n",
        "    to_page=2,\n",
        "    max_workers=5,\n",
        ")\n",
        "```\n",
        "\n",
        "## Edge Cases Handled:\n",
        "- New models without reviews\n",
        "- Missing seller links\n",
        "- Variable HTML structures\n",
        "- Network timeouts\n",
        "- Incomplete listings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvCOvzAi-zIL",
        "outputId": "d3d9a131-e949-4f6c-c6ba-71ccff086da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPDKiaBFQBol",
        "outputId": "b7fad055-43c9-4b63-e011-7d48c82ab912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
            "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.10.5 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.10.5)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
            "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.3.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.38.0 trio-0.32.0 trio-websocket-0.12.2 wsproto-1.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8q9I5zTQxrk",
        "outputId": "b7720cf2-1f6a-41ab-b47e-37934b59532b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,982 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,530 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,451 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,130 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,858 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,830 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,173 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,595 kB]\n",
            "Fetched 37.2 MB in 11s (3,514 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 34.3 MB of archives.\n",
            "After this operation, 135 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.17 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.17 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.72+ubuntu22.04 [31.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.6 [3,668 B]\n",
            "Fetched 34.3 MB in 1s (41.7 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 121235 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 121435 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.72+ubuntu22.04_amd64.deb ...\n",
            "Unpacking snapd (2.72+ubuntu22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.17) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.72+ubuntu22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 121667 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.6_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.6) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.6) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.17) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "# Install Chrome and ChromeDriver (standalone version, no snap)\n",
        "!apt-get update\n",
        "!apt-get install -y wget unzip\n",
        "\n",
        "# Download and install Chrome\n",
        "!wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -y ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "# Verify installation\n",
        "!which google-chrome\n",
        "!google-chrome --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Chrome installation\n",
        "print(\"Testing Chrome installation...\")\n",
        "!which google-chrome\n",
        "!google-chrome --version\n",
        "print(\"\\n✓ Chrome is installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test to verify Chrome driver works\n",
        "print(\"=\" * 50)\n",
        "print(\"TESTING CHROME DRIVER INITIALIZATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    test_driver = init_driver(headless=True)\n",
        "    print(\"\\n✅ SUCCESS! Chrome driver is working!\")\n",
        "    print(\"Attempting to navigate to Google...\")\n",
        "    test_driver.get(\"https://www.google.com\")\n",
        "    print(f\"✅ Page title: {test_driver.title}\")\n",
        "    test_driver.quit()\n",
        "    print(\"✅ All tests passed!\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ FAILED: {e}\\n\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeU8oNIJv1bL"
      },
      "source": [
        "---\n",
        "\n",
        "## Code Structure\n",
        "\n",
        "This notebook is organized into the following sections:\n",
        "\n",
        "1. **Environment Setup** (Cells 2-3)\n",
        "   - Install Selenium\n",
        "   - Install Chrome driver\n",
        "\n",
        "2. **Library Imports** (Cell 4)\n",
        "   - All required imports in one place\n",
        "   - Organized by: Standard library → Third-party → Selenium\n",
        "\n",
        "3. **Core Utility Functions** (Cell 5)\n",
        "   - `init_driver()` - Initialize browser\n",
        "   - `clean_and_convert_to_int()` - Data cleaning\n",
        "   - `get_html_with_driver()` - Fetch HTML\n",
        "   - `save_html()` / `load_html()` - HTML caching\n",
        "   - `get_soup_from_html()` - Parse HTML\n",
        "\n",
        "4. **HTML Parsing Functions** (Cell 5)\n",
        "   - `_parse_listing_data()` - Parse car listing\n",
        "   - `_parse_seller_data()` - Parse seller info\n",
        "   - `_parse_reviews_data()` - Parse reviews\n",
        "\n",
        "5. **Scraping Functions** (Cell 5)\n",
        "   - `scrape_*_from_html()` - Parse from HTML string (NEW)\n",
        "   - `scrape_*_website()` - Direct scraping (DEPRECATED)\n",
        "   - `collect_html_batch()` - Batch HTML collection (NEW)\n",
        "   - `scrape_full_info()` - Complete scraping\n",
        "\n",
        "6. **URL Crawling Functions** (Cell 6)\n",
        "   - `crawl_all_listing_urls()` - Collect car URLs from search pages\n",
        "\n",
        "7. **Batch Scraping Functions** (Cell 7)\n",
        "   - `scrape_from_url_files_batch()` - NEW batch method (RECOMMENDED)\n",
        "   - `scrape_from_url_files()` - Old threaded method (DEPRECATED)\n",
        "\n",
        "8. **Testing Cells** (Cells 8-11)\n",
        "   - Test individual URLs\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE9BaOieMl91"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Any, Dict, List, Optional, Set\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Third-party imports\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Selenium imports\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.remote.webdriver import WebDriver\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "\n",
        "# WebDriver Manager (for auto-downloading correct ChromeDriver)\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from webdriver_manager.core.os_manager import ChromeType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCf-kZ2sUSkh"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CORE UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def init_driver(headless: bool = True) -> webdriver.Chrome:\n",
        "    \"\"\"Initializes and configures the Selenium Chrome WebDriver for Colab.\"\"\"\n",
        "    chrome_options = Options()\n",
        "    \n",
        "    # Set binary location to Google Chrome (not chromium)\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome'\n",
        "\n",
        "    if headless:\n",
        "        chrome_options.add_argument('--headless=new')  # Use new headless mode\n",
        "\n",
        "    # Essential arguments for Colab environment\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-gpu')\n",
        "    chrome_options.add_argument('--disable-software-rasterizer')\n",
        "    chrome_options.add_argument('--disable-extensions')\n",
        "    chrome_options.add_argument('--disable-setuid-sandbox')\n",
        "    chrome_options.add_argument('--remote-debugging-port=9222')  # Important for Colab\n",
        "    chrome_options.add_argument('--ignore-certificate-errors')\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
        "    chrome_options.add_argument('--window-size=1920,1080')\n",
        "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36')\n",
        "    \n",
        "    # Additional preferences to prevent crashes\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "    \n",
        "    try:\n",
        "        print(\"Initializing Chrome driver with ChromeDriverManager...\")\n",
        "        # Use ChromeDriverManager to auto-download matching ChromeDriver for Google Chrome\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        print(\"✓ Chrome driver initialized successfully\")\n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed with ChromeDriverManager: {e}\")\n",
        "        raise Exception(f\"Could not initialize Chrome driver: {e}\")\n",
        "\n",
        "def clean_and_convert_to_int(text: Optional[str]) -> Optional[int]:\n",
        "    \"\"\"\n",
        "    Removes all non-digit characters from a string and converts it to an integer.\n",
        "\n",
        "    Args:\n",
        "        text (str | None): The input string to clean (e.g., \"$25,999\", \"(38,191 mi.)\").\n",
        "\n",
        "    Returns:\n",
        "        int | None: The cleaned integer, or None if the input is empty or invalid.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Use regex to remove any character that is not a digit\n",
        "        cleaned_string = re.sub(r'\\D', '', text)\n",
        "        return int(cleaned_string)\n",
        "    except (ValueError, TypeError):\n",
        "        print(f\"Warning: Could not convert '{text}' to an integer.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_html_with_driver(driver: webdriver.Chrome, url: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Get raw HTML using existing driver instance.\n",
        "\n",
        "    Args:\n",
        "        driver: Existing Chrome WebDriver instance.\n",
        "        url: The URL of the page to scrape.\n",
        "\n",
        "    Returns:\n",
        "        Raw HTML string or None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Navigating to {url}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        driver.get(url)\n",
        "        print(f\"Page loaded in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        time.sleep(random.uniform(0.5, 1.5))\n",
        "\n",
        "        return driver.page_source\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def save_html(html_content: str, output_path: str) -> bool:\n",
        "    \"\"\"Save HTML content to file.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(html_content)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving HTML to {output_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def load_html(html_path: str) -> Optional[str]:\n",
        "    \"\"\"Load HTML content from file.\"\"\"\n",
        "    try:\n",
        "        with open(html_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading HTML from {html_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_soup_from_html(html_content: str) -> Optional[BeautifulSoup]:\n",
        "    \"\"\"Parse HTML string to BeautifulSoup object.\"\"\"\n",
        "    try:\n",
        "        return BeautifulSoup(html_content, \"html.parser\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# HTML PARSING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def _parse_listing_data(soup: BeautifulSoup, url: str = \"\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parse car listing data from HTML soup with robust error handling.\n",
        "    Each field wrapped in try-catch to continue on errors.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    def safe_get_text(element, default=None):\n",
        "        \"\"\"Safely extract text from element.\"\"\"\n",
        "        try:\n",
        "            return element.get_text(strip=True) if element else default\n",
        "        except:\n",
        "            return default\n",
        "\n",
        "    def safe_get_attr(element, attr, default=None):\n",
        "        \"\"\"Safely get attribute from element.\"\"\"\n",
        "        try:\n",
        "            return element.get(attr) if element and hasattr(element, 'get') else default\n",
        "        except:\n",
        "            return default\n",
        "\n",
        "    # POST DATA\n",
        "    post = {}\n",
        "\n",
        "    # Each field wrapped in try-catch\n",
        "    try:\n",
        "        post[\"new_used\"] = safe_get_text(soup.select_one(\"p.new-used\"))\n",
        "    except:\n",
        "        post[\"new_used\"] = None\n",
        "\n",
        "    try:\n",
        "        post[\"title\"] = safe_get_text(soup.select_one(\"h1.listing-title\"))\n",
        "    except:\n",
        "        post[\"title\"] = None\n",
        "\n",
        "    try:\n",
        "        mileage_text = safe_get_text(soup.select_one(\"p.listing-mileage\"))\n",
        "        post[\"mileage\"] = clean_and_convert_to_int(mileage_text)\n",
        "    except:\n",
        "        post[\"mileage\"] = None\n",
        "\n",
        "    try:\n",
        "        price_text = safe_get_text(soup.select_one(\"span.primary-price\"))\n",
        "        post[\"price\"] = clean_and_convert_to_int(price_text)\n",
        "    except:\n",
        "        post[\"price\"] = None\n",
        "\n",
        "    try:\n",
        "        payment_button = soup.select_one('spark-button.monthly-payment-est-link')\n",
        "        payment_value = safe_get_attr(payment_button, 'phx-value-monthly-payment')\n",
        "        post[\"monthly_payment\"] = clean_and_convert_to_int(payment_value)\n",
        "    except:\n",
        "        post[\"monthly_payment\"] = None\n",
        "\n",
        "    # Basics (Engine, Transmission, etc.)\n",
        "    try:\n",
        "        basic_dict = {}\n",
        "        basics_terms = soup.select_one('section.sds-page-section.basics-section dl.fancy-description-list')\n",
        "        if basics_terms:\n",
        "            for term in basics_terms.find_all('dt', recursive=False):\n",
        "                try:\n",
        "                    key = safe_get_text(term)\n",
        "                    value_tag = term.find_next_sibling('dd')\n",
        "                    if key and value_tag:\n",
        "                        if key == 'MPG':\n",
        "                            mpg_span = value_tag.find('span', attrs={'slot': 'trigger'})\n",
        "                            value = safe_get_text(mpg_span)\n",
        "                        else:\n",
        "                            value = safe_get_text(value_tag)\n",
        "                        basic_dict[key] = value\n",
        "                except:\n",
        "                    continue\n",
        "        post[\"basics_des\"] = basic_dict if basic_dict else None\n",
        "    except:\n",
        "        post[\"basics_des\"] = None\n",
        "\n",
        "    # Features\n",
        "    try:\n",
        "        feature_dict = {}\n",
        "        feature_terms = soup.select_one('section.sds-page-section.features-section dl.fancy-description-list')\n",
        "        if feature_terms:\n",
        "            for term in feature_terms.find_all('dt', recursive=False):\n",
        "                try:\n",
        "                    key = safe_get_text(term)\n",
        "                    value_tag = term.find_next_sibling('dd')\n",
        "                    if key and value_tag:\n",
        "                        value = safe_get_text(value_tag, default='')\n",
        "                        value_list = value.replace('\\n\\n', '').replace('\\n', ',').split(\",\")\n",
        "                        feature_dict[key] = [v.strip() for v in value_list if v.strip()]\n",
        "                except:\n",
        "                    continue\n",
        "        post[\"feature_des\"] = feature_dict if feature_dict else None\n",
        "    except:\n",
        "        post[\"feature_des\"] = None\n",
        "\n",
        "    # User History\n",
        "    try:\n",
        "        user_history_dict = {}\n",
        "        user_history_terms = soup.select_one('section.sds-page-section.vehicle-history-section dl.fancy-description-list')\n",
        "        if user_history_terms:\n",
        "            for term in user_history_terms.find_all('dt', recursive=False):\n",
        "                try:\n",
        "                    key = safe_get_text(term)\n",
        "                    value_tag = term.find_next_sibling('dd')\n",
        "                    if key and value_tag:\n",
        "                        user_history_dict[key] = safe_get_text(value_tag)\n",
        "                except:\n",
        "                    continue\n",
        "        post[\"user_history_des\"] = user_history_dict if user_history_dict else None\n",
        "    except:\n",
        "        post[\"user_history_des\"] = None\n",
        "\n",
        "    # Warranty\n",
        "    try:\n",
        "        warranty_dict = {}\n",
        "        warranty_terms = soup.select_one('section.sds-page-section.warranty_section dl.fancy-description-list')\n",
        "        if warranty_terms:\n",
        "            for term in warranty_terms.find_all('dt', recursive=False):\n",
        "                try:\n",
        "                    key = safe_get_text(term)\n",
        "                    value_tag = term.find_next_sibling('dd')\n",
        "                    if key and value_tag:\n",
        "                        value = safe_get_text(value_tag)\n",
        "                        warranty_dict[key] = None if value in ['–', '—'] else value\n",
        "                except:\n",
        "                    continue\n",
        "        post[\"warranty_des\"] = warranty_dict if warranty_dict else None\n",
        "    except:\n",
        "        post[\"warranty_des\"] = None\n",
        "\n",
        "    # Images\n",
        "    try:\n",
        "        image_terms = soup.select_one('gallery-slides')\n",
        "        if image_terms:\n",
        "            images = image_terms.find_all('img', recursive=False)\n",
        "            image_urls = [safe_get_attr(img, 'src') for img in images]\n",
        "            post['image'] = [url for url in image_urls if url]\n",
        "        else:\n",
        "            post['image'] = None\n",
        "    except:\n",
        "        post['image'] = None\n",
        "\n",
        "    data['post'] = post\n",
        "\n",
        "    # SELLER DATA\n",
        "    seller = {}\n",
        "    try:\n",
        "        seller_name_tag = soup.select_one('h3.spark-heading-5.heading.seller-name')\n",
        "        seller['seller_name'] = safe_get_text(seller_name_tag)\n",
        "    except:\n",
        "        seller['seller_name'] = None\n",
        "\n",
        "    try:\n",
        "        seller_link_tag = soup.select_one('a.sds-rating__link.sds-button-link')\n",
        "        seller_link = safe_get_attr(seller_link_tag, 'href')\n",
        "\n",
        "        if seller_link:\n",
        "            seller_link = 'https://www.cars.com' + seller_link\n",
        "            seller_key = seller_link.split('/')[-2] if '/' in seller_link else None\n",
        "        else:\n",
        "            seller_key = None\n",
        "\n",
        "        seller['seller_link'] = seller_link\n",
        "        seller['seller_key'] = seller_key\n",
        "    except:\n",
        "        seller['seller_link'] = None\n",
        "        seller['seller_key'] = None\n",
        "\n",
        "    data['seller'] = seller\n",
        "\n",
        "    # CAR MODEL DATA\n",
        "    car = {}\n",
        "    try:\n",
        "        car_link_tag = soup.select_one('div.mmy-page-link a')\n",
        "\n",
        "        if car_link_tag:\n",
        "            car['car_model'] = safe_get_attr(car_link_tag, 'data-slugs')\n",
        "            car_link = safe_get_attr(car_link_tag, 'href')\n",
        "\n",
        "            if car_link:\n",
        "                car_link = 'https://www.cars.com' + car_link\n",
        "                review_link = car_link + 'consumer-reviews/?page_size=200'\n",
        "            else:\n",
        "                review_link = None\n",
        "\n",
        "            car['car_link'] = car_link\n",
        "            car['review_link'] = review_link\n",
        "        else:\n",
        "            car['car_model'] = None\n",
        "            car['car_link'] = None\n",
        "            car['review_link'] = None\n",
        "    except:\n",
        "        car['car_model'] = None\n",
        "        car['car_link'] = None\n",
        "        car['review_link'] = None\n",
        "\n",
        "    # Car rating\n",
        "    try:\n",
        "        car_rating_tag = soup.select_one('div.vehicle-reviews spark-rating')\n",
        "        car['car_rating'] = safe_get_attr(car_rating_tag, 'rating')\n",
        "    except:\n",
        "        car['car_rating'] = None\n",
        "\n",
        "    # Rating breakdown\n",
        "    try:\n",
        "        ratings = {}\n",
        "        car_terms = soup.select_one('div.review-breakdown ul.sds-definition-list.review-breakdown--list')\n",
        "        if car_terms:\n",
        "            for li in car_terms.select(\"li\"):\n",
        "                try:\n",
        "                    name_tag = li.select_one(\".sds-definition-list__display-name\")\n",
        "                    value_tag = li.select_one(\".sds-definition-list__value\")\n",
        "                    if name_tag and value_tag:\n",
        "                        name = safe_get_text(name_tag)\n",
        "                        value = safe_get_text(value_tag)\n",
        "                        try:\n",
        "                            ratings[name] = float(value)\n",
        "                        except (ValueError, TypeError):\n",
        "                            ratings[name] = value\n",
        "                except:\n",
        "                    continue\n",
        "        car['ratings'] = ratings if ratings else None\n",
        "    except:\n",
        "        car['ratings'] = None\n",
        "\n",
        "    # Percentage recommend\n",
        "    try:\n",
        "        percentage_elem = soup.select_one('div.reviews-recommended')\n",
        "        if percentage_elem:\n",
        "            percentage_text = safe_get_text(percentage_elem, default='')\n",
        "            if percentage_text:\n",
        "                parts = percentage_text.split(' ')\n",
        "                first_part = parts[0] if parts else ''\n",
        "                try:\n",
        "                    car['percentage_recommend'] = float(first_part.rstrip('%')) if first_part else None\n",
        "                except (ValueError, TypeError):\n",
        "                    car['percentage_recommend'] = None\n",
        "            else:\n",
        "                car['percentage_recommend'] = None\n",
        "        else:\n",
        "            car['percentage_recommend'] = None\n",
        "    except:\n",
        "        car['percentage_recommend'] = None\n",
        "\n",
        "    data['car'] = car\n",
        "\n",
        "    # Metadata for tracking data completeness\n",
        "    data['_metadata'] = {\n",
        "        'url': url,\n",
        "        'has_car_link': bool(car_link_tag),\n",
        "        'has_ratings': bool(car_rating_tag),\n",
        "        'has_percentage': bool(percentage_elem),\n",
        "        'is_complete': all([\n",
        "            post.get('title'),\n",
        "            seller.get('seller_name'),\n",
        "            post.get('price')\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    return data\n",
        "\n",
        "def _parse_reviews_data(soup: BeautifulSoup, data: Dict[str, Dict]) -> Dict[str, Dict]:\n",
        "    \"\"\"Parse review data with try-catch for each field.\"\"\"\n",
        "    output_data = copy.deepcopy(data)\n",
        "\n",
        "    def safe_get_text(element, default=None):\n",
        "        try:\n",
        "            return element.get_text(strip=True) if element else default\n",
        "        except:\n",
        "            return default\n",
        "\n",
        "    # Car Brand and Name\n",
        "    try:\n",
        "        car_name_elem = soup.select_one('div.sds-page-section.vehicle-reviews-page h1')\n",
        "        if car_name_elem:\n",
        "            output_data['car']['car_name'] = car_name_elem.get_text(strip=True).replace(' consumer reviews', '')\n",
        "        else:\n",
        "            output_data['car']['car_name'] = None\n",
        "    except:\n",
        "        output_data['car']['car_name'] = None\n",
        "\n",
        "    try:\n",
        "        brand_elem = soup.select_one('a[data-linkname=\"research-make\"]')\n",
        "        output_data['car']['brand'] = safe_get_text(brand_elem)\n",
        "    except:\n",
        "        output_data['car']['brand'] = None\n",
        "\n",
        "    # Reviews\n",
        "    try:\n",
        "        listing_reviews_terms = soup.select('div.sds-container.consumer-review-container')\n",
        "        reviews = []\n",
        "\n",
        "        if listing_reviews_terms:\n",
        "            for review_term in listing_reviews_terms:\n",
        "                try:\n",
        "                    # Skip if no review body\n",
        "                    if not review_term.select_one('p.review-body'):\n",
        "                        continue\n",
        "\n",
        "                    review_data = {}\n",
        "\n",
        "                    # Overall Rating\n",
        "                    try:\n",
        "                        overall_rating_tag = review_term.select_one('spark-rating')\n",
        "                        review_data['overall_rating'] = float(overall_rating_tag.get('rating')) if overall_rating_tag and overall_rating_tag.get('rating') else None\n",
        "                    except:\n",
        "                        review_data['overall_rating'] = None\n",
        "\n",
        "                    # Time\n",
        "                    try:\n",
        "                        time_tag = review_term.select_one('.review-byline.review-section > div:nth-child(1)')\n",
        "                        review_data['time'] = safe_get_text(time_tag)\n",
        "                    except:\n",
        "                        review_data['time'] = None\n",
        "\n",
        "                    # User Name and Location\n",
        "                    try:\n",
        "                        byline_tag = review_term.select_one('.review-byline.review-section > div:nth-child(2)')\n",
        "                        if byline_tag:\n",
        "                            byline_text = byline_tag.get_text(strip=True)\n",
        "                            try:\n",
        "                                username = byline_text.split('By ')[1].split(' from ')[0]\n",
        "                                review_data['user_name'] = username\n",
        "                                review_data['from'] = byline_text.split(' from ')[1].strip()\n",
        "                            except (IndexError, AttributeError):\n",
        "                                review_data['user_name'] = byline_text\n",
        "                                review_data['from'] = None\n",
        "                        else:\n",
        "                            review_data['user_name'] = None\n",
        "                            review_data['from'] = None\n",
        "                    except:\n",
        "                        review_data['user_name'] = None\n",
        "                        review_data['from'] = None\n",
        "\n",
        "                    # Review Body\n",
        "                    try:\n",
        "                        review_body_tag = review_term.select_one('p.review-body')\n",
        "                        review_data['review'] = safe_get_text(review_body_tag)\n",
        "                    except:\n",
        "                        review_data['review'] = None\n",
        "\n",
        "                    # Rating Breakdown\n",
        "                    try:\n",
        "                        ratings_breakdown = {}\n",
        "                        breakdown_list = review_term.select_one('.review-breakdown--list')\n",
        "                        if breakdown_list:\n",
        "                            for item in breakdown_list.select('li'):\n",
        "                                try:\n",
        "                                    key_tag = item.select_one('.sds-definition-list__display-name')\n",
        "                                    value_tag = item.select_one('.sds-definition-list__value')\n",
        "                                    if key_tag and value_tag:\n",
        "                                        key = key_tag.get_text(strip=True)\n",
        "                                        try:\n",
        "                                            value = float(value_tag.get_text(strip=True))\n",
        "                                        except (ValueError, AttributeError):\n",
        "                                            value = value_tag.get_text(strip=True)\n",
        "                                        ratings_breakdown[key] = value\n",
        "                                except:\n",
        "                                    continue\n",
        "                        review_data['ratings_breakdown'] = ratings_breakdown\n",
        "                    except:\n",
        "                        review_data['ratings_breakdown'] = {}\n",
        "\n",
        "                    reviews.append(review_data)\n",
        "\n",
        "                except:\n",
        "                    # Skip this review if any error\n",
        "                    continue\n",
        "\n",
        "        output_data['car']['reviews'] = reviews if reviews else None\n",
        "    except:\n",
        "        output_data['car']['reviews'] = None\n",
        "\n",
        "    return output_data\n",
        "\n",
        "def _parse_seller_data(soup: BeautifulSoup, data: Dict[str, Dict]) -> Dict[str, Dict]:\n",
        "    \"\"\"Parse seller data with try-catch for each field.\"\"\"\n",
        "    output_data = copy.deepcopy(data)\n",
        "\n",
        "    def safe_get_text(element, default=None):\n",
        "        try:\n",
        "            return element.get_text(strip=True) if element else default\n",
        "        except:\n",
        "            return default\n",
        "\n",
        "    # Phone\n",
        "    try:\n",
        "        phone_data = {}\n",
        "        phone_terms = soup.select('div.dealer-phone')\n",
        "        for phone in phone_terms:\n",
        "            try:\n",
        "                title_tag = phone.select_one('span.phone-number-title')\n",
        "                phone_type = title_tag.get_text(strip=True) if title_tag else 'Unknown'\n",
        "\n",
        "                number_tag = phone.select_one('a.phone-number')\n",
        "                phone_number = number_tag.get_text(strip=True) if number_tag else None\n",
        "\n",
        "                if phone_type and phone_number:\n",
        "                    phone_data[phone_type] = phone_number\n",
        "            except:\n",
        "                continue\n",
        "        output_data['seller']['phone_info'] = phone_data\n",
        "    except:\n",
        "        output_data['seller']['phone_info'] = {}\n",
        "\n",
        "    # Address\n",
        "    try:\n",
        "        destination = soup.select_one('div.dealer-address')\n",
        "        output_data['seller']['destination'] = safe_get_text(destination)\n",
        "    except:\n",
        "        output_data['seller']['destination'] = None\n",
        "\n",
        "    # Hours\n",
        "    try:\n",
        "        hours_data = {}\n",
        "        rows = soup.select('table.dealer-hours tr')\n",
        "        for row in rows:\n",
        "            try:\n",
        "                cells = row.find_all('td')\n",
        "                if len(cells) == 2:\n",
        "                    key = cells[0].get_text(strip=True).replace(':', '')\n",
        "                    value = cells[1].get_text(strip=True)\n",
        "                    hours_data[key] = value\n",
        "            except:\n",
        "                continue\n",
        "        output_data['seller']['hours'] = hours_data\n",
        "    except:\n",
        "        output_data['seller']['hours'] = {}\n",
        "\n",
        "    # Rating\n",
        "    try:\n",
        "        rating_terms = soup.select_one('div.dealer-info-section spark-rating')\n",
        "        if rating_terms:\n",
        "            rating = float(rating_terms['rating'])\n",
        "            rating_counts = clean_and_convert_to_int(rating_terms.select_one('span.test1.sds-rating__link.sds-button-link').get_text(strip=True))\n",
        "        else:\n",
        "            rating = None\n",
        "            rating_counts = None\n",
        "        output_data['seller']['seller_rating'] = rating\n",
        "        output_data['seller']['seller_rating_count'] = rating_counts\n",
        "    except:\n",
        "        output_data['seller']['seller_rating'] = None\n",
        "        output_data['seller']['seller_rating_count'] = None\n",
        "\n",
        "    # Description\n",
        "    try:\n",
        "        description_terms = soup.select_one('div.dealer-description.scrubbed-html')\n",
        "        output_data['seller']['description'] = safe_get_text(description_terms)\n",
        "    except:\n",
        "        output_data['seller']['description'] = None\n",
        "\n",
        "    # Images\n",
        "    try:\n",
        "        image_terms = soup.select('div.media-gallery-section img')\n",
        "        if image_terms:\n",
        "            image_urls = []\n",
        "            for image in image_terms:\n",
        "                try:\n",
        "                    image_urls.append(image['src'])\n",
        "                except:\n",
        "                    continue\n",
        "            output_data['seller']['images'] = image_urls\n",
        "        else:\n",
        "            output_data['seller']['images'] = None\n",
        "    except:\n",
        "        output_data['seller']['images'] = None\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# ============================================================================\n",
        "# SCRAPING FUNCTIONS (New & Deprecated)\n",
        "# ============================================================================\n",
        "\n",
        "def scrape_post_from_html(html_content: str, url: str = \"\") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse car listing data from HTML content.\n",
        "\n",
        "    Args:\n",
        "        html_content: Raw HTML string.\n",
        "        url: Original URL (for metadata).\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing scraped data, or None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = get_soup_from_html(html_content)\n",
        "        if not soup:\n",
        "            return None\n",
        "        return _parse_listing_data(soup, url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing listing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def scrape_post_website(url: str, headless: bool = True) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Scrape car listing data from a Cars.com listing page.\n",
        "    DEPRECATED: Use batch HTML collection instead.\n",
        "    \"\"\"\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = init_driver(headless=headless)\n",
        "        html_content = get_html_with_driver(driver, url)\n",
        "        if not html_content:\n",
        "            return None\n",
        "        return scrape_post_from_html(html_content, url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping listing {url}: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "def scrape_seller_from_html(html_content: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Parse seller data from HTML content.\"\"\"\n",
        "    try:\n",
        "        soup = get_soup_from_html(html_content)\n",
        "        if not soup:\n",
        "            return data\n",
        "        return _parse_seller_data(soup, data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing seller data: {e}\")\n",
        "        return data\n",
        "\n",
        "\n",
        "def scrape_review_from_html(html_content: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Parse review data from HTML content.\"\"\"\n",
        "    try:\n",
        "        soup = get_soup_from_html(html_content)\n",
        "        if not soup:\n",
        "            return data\n",
        "        return _parse_reviews_data(soup, data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing review data: {e}\")\n",
        "        return data\n",
        "\n",
        "\n",
        "def scrape_seller_website(url: str, data: Optional[Dict[str, Any]], headless: bool = True) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Scrape seller data from a dealer page.\n",
        "    DEPRECATED: Use batch HTML collection instead.\n",
        "    \"\"\"\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = init_driver(headless=headless)\n",
        "        html_content = get_html_with_driver(driver, url)\n",
        "        if not html_content:\n",
        "            return data\n",
        "        return scrape_seller_from_html(html_content, data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping seller {url}: {e}\")\n",
        "        return data\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "\n",
        "def scrape_review_website(url: str, data: Optional[Dict[str, Any]], headless: bool = True) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Scrape car reviews from a reviews page.\n",
        "    DEPRECATED: Use batch HTML collection instead.\n",
        "    \"\"\"\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = init_driver(headless=headless)\n",
        "        html_content = get_html_with_driver(driver, url)\n",
        "        if not html_content:\n",
        "            return data\n",
        "        return scrape_review_from_html(html_content, data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping reviews {url}: {e}\")\n",
        "        return data\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "\n",
        "def collect_html_batch(urls: List[str], html_dir: str, headless: bool = True) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Collect HTML for multiple URLs using single browser instance.\n",
        "    Saves HTML files and returns mapping of URL to HTML file path.\n",
        "\n",
        "    Args:\n",
        "        urls: List of URLs to scrape.\n",
        "        html_dir: Directory to save HTML files.\n",
        "        headless: Whether to run browser in headless mode.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping URL to HTML file path.\n",
        "    \"\"\"\n",
        "    os.makedirs(html_dir, exist_ok=True)\n",
        "    url_to_html_path = {}\n",
        "\n",
        "    driver = init_driver(headless=headless)\n",
        "    try:\n",
        "        for idx, url in enumerate(urls, 1):\n",
        "            try:\n",
        "                print(f\"[{idx}/{len(urls)}] Collecting HTML: {url}\")\n",
        "\n",
        "                html_content = get_html_with_driver(driver, url)\n",
        "                if not html_content:\n",
        "                    print(f\"  Failed to get HTML\")\n",
        "                    continue\n",
        "\n",
        "                # Generate filename from URL\n",
        "                url_hash = str(abs(hash(url)))[:10]\n",
        "                html_filename = f\"{idx}_{url_hash}.html\"\n",
        "                html_path = os.path.join(html_dir, html_filename)\n",
        "\n",
        "                if save_html(html_content, html_path):\n",
        "                    url_to_html_path[url] = html_path\n",
        "                    print(f\"  Saved -> {html_path}\")\n",
        "\n",
        "                time.sleep(random.uniform(0.3, 0.8))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "                continue\n",
        "    finally:\n",
        "        print(\"Closing browser...\")\n",
        "        driver.quit()\n",
        "\n",
        "    return url_to_html_path\n",
        "\n",
        "\n",
        "def scrape_full_info_from_html(listing_html: str, seller_html: Optional[str], review_html: Optional[str], url: str = \"\") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse complete car information from saved HTML files.\n",
        "\n",
        "    Args:\n",
        "        listing_html: HTML content of listing page.\n",
        "        seller_html: HTML content of seller page (optional).\n",
        "        review_html: HTML content of review page (optional).\n",
        "        url: Original listing URL (for metadata).\n",
        "\n",
        "    Returns:\n",
        "        Complete data dictionary, or None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse listing\n",
        "        scraped_data = scrape_post_from_html(listing_html, url)\n",
        "        if not scraped_data:\n",
        "            print(f\"Failed to parse listing data for {url}\")\n",
        "            return None\n",
        "\n",
        "        # Parse seller (if available)\n",
        "        if seller_html:\n",
        "            try:\n",
        "                scraped_data = scrape_seller_from_html(seller_html, scraped_data)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not parse seller data: {e}\")\n",
        "\n",
        "        # Parse reviews (if available)\n",
        "        if review_html:\n",
        "            try:\n",
        "                scraped_data = scrape_review_from_html(review_html, scraped_data)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not parse review data: {e}\")\n",
        "\n",
        "        return scraped_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error parsing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def scrape_full_info(url: str, headless: bool = True) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Scrape complete car information including listing, seller, and reviews.\n",
        "    DEPRECATED: Use batch HTML collection + parsing instead for better performance.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        scraped_data = scrape_post_website(url, headless=headless)\n",
        "        if not scraped_data:\n",
        "            print(f\"Failed to scrape listing data for {url}\")\n",
        "            return None\n",
        "\n",
        "        if scraped_data.get('seller', {}).get('seller_link'):\n",
        "            try:\n",
        "                scraped_data = scrape_seller_website(\n",
        "                    scraped_data['seller']['seller_link'],\n",
        "                    scraped_data,\n",
        "                    headless=headless\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Could not scrape seller data: {e}\")\n",
        "\n",
        "        if scraped_data.get('car', {}).get('review_link'):\n",
        "            try:\n",
        "                scraped_data = scrape_review_website(\n",
        "                    scraped_data['car']['review_link'],\n",
        "                    scraped_data,\n",
        "                    headless=headless\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Could not scrape review data: {e}\")\n",
        "        else:\n",
        "            print(f\"No review link available - skipping reviews\")\n",
        "\n",
        "        return scraped_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error scraping {url}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "E_DDsJvZ4cc3",
        "outputId": "6962db4d-3fed-4ab7-bc86-355cffcca3d1"
      },
      "outputs": [
        {
          "ename": "SessionNotCreatedException",
          "evalue": "Message: session not created: Chrome instance exited. Examine ChromeDriver verbose log to determine the cause.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\nStacktrace:\n#0 0x5a200764332a <unknown>\n#1 0x5a200708fe4b <unknown>\n#2 0x5a20070ca919 <unknown>\n#3 0x5a20070c6375 <unknown>\n#4 0x5a2007117016 <unknown>\n#5 0x5a2007116736 <unknown>\n#6 0x5a20070d4c1a <unknown>\n#7 0x5a20070d5921 <unknown>\n#8 0x5a200760a239 <unknown>\n#9 0x5a200760d1e8 <unknown>\n#10 0x5a20075f34c9 <unknown>\n#11 0x5a200760ddb5 <unknown>\n#12 0x5a20075dae93 <unknown>\n#13 0x5a2007630098 <unknown>\n#14 0x5a2007630273 <unknown>\n#15 0x5a20076422c3 <unknown>\n#16 0x7da610259ac3 <unknown>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1625447698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mstart_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m170\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mend_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     crawl_all_listing_urls(\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mstart_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mend_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_page\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1625447698.py\u001b[0m in \u001b[0;36mcrawl_all_listing_urls\u001b[0;34m(start_page, end_page, output_dir, delay_between_pages, headless, driver_path, clear_output_first)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheadless\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_page\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1625447698.py\u001b[0m in \u001b[0;36minit_driver\u001b[0;34m(headless, driver_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mbrowser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mvendor_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fedcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFedCM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"capabilities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: Chrome instance exited. Examine ChromeDriver verbose log to determine the cause.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\nStacktrace:\n#0 0x5a200764332a <unknown>\n#1 0x5a200708fe4b <unknown>\n#2 0x5a20070ca919 <unknown>\n#3 0x5a20070c6375 <unknown>\n#4 0x5a2007117016 <unknown>\n#5 0x5a2007116736 <unknown>\n#6 0x5a20070d4c1a <unknown>\n#7 0x5a20070d5921 <unknown>\n#8 0x5a200760a239 <unknown>\n#9 0x5a200760d1e8 <unknown>\n#10 0x5a20075f34c9 <unknown>\n#11 0x5a200760ddb5 <unknown>\n#12 0x5a20075dae93 <unknown>\n#13 0x5a2007630098 <unknown>\n#14 0x5a2007630273 <unknown>\n#15 0x5a20076422c3 <unknown>\n#16 0x7da610259ac3 <unknown>\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# URL CRAWLING FUNCTIONS (For collecting car listing URLs from search pages)\n",
        "# ============================================================================\n",
        "\n",
        "# Constants for URL crawling\n",
        "RESULTS_URL_TMPL = (\n",
        "    \"https://www.cars.com/shopping/results/?\"\n",
        "    \"list_price_max=&makes[]=&maximum_distance=all&models[]=&page={page}&stock_type=all&zip=60606\"\n",
        ")\n",
        "SITE_BASE = \"https://www.cars.com\"\n",
        "\n",
        "\n",
        "def init_driver(headless: bool = True, driver_path: Optional[str] = None) -> webdriver.Chrome:\n",
        "    \"\"\"Initialize Selenium Chrome WebDriver with sensible defaults for Colab.\"\"\"\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    \n",
        "    chrome_options = Options()\n",
        "    \n",
        "    # Set binary location to Google Chrome (not chromium)\n",
        "    chrome_options.binary_location = '/usr/bin/google-chrome'\n",
        "    \n",
        "    if headless:\n",
        "        chrome_options.add_argument(\"--headless=new\")  # modern headless\n",
        "    \n",
        "    # Essential arguments for Colab environment\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--disable-software-rasterizer\")\n",
        "    chrome_options.add_argument(\"--disable-extensions\")\n",
        "    chrome_options.add_argument(\"--disable-setuid-sandbox\")\n",
        "    chrome_options.add_argument(\"--remote-debugging-port=9222\")  # Important for Colab\n",
        "    chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
        "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    chrome_options.add_argument(\n",
        "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "    \n",
        "    # Additional preferences to prevent crashes\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "    try:\n",
        "        print(\"Initializing Chrome driver with ChromeDriverManager...\")\n",
        "        # Use ChromeDriverManager to auto-download matching ChromeDriver for Google Chrome\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        print(\"✓ Chrome driver initialized successfully\")\n",
        "        return driver\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed with ChromeDriverManager: {e}\")\n",
        "        raise Exception(f\"Could not initialize Chrome driver: {e}\")\n",
        "\n",
        "\n",
        "def wait_for_cards(driver: webdriver.Chrome, timeout: int = 15) -> None:\n",
        "    \"\"\"Wait until at least one vehicle card link is present in the DOM.\"\"\"\n",
        "    WebDriverWait(driver, timeout).until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \"a.vehicle-card-link\"))\n",
        "    )\n",
        "\n",
        "\n",
        "def scroll_to_bottom(driver: webdriver.Chrome, pause: float = 0.6, max_rounds: int = 12) -> None:\n",
        "    \"\"\"Scroll down to trigger lazy loading.\"\"\"\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    rounds = 0\n",
        "    while rounds < max_rounds:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(pause)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "        rounds += 1\n",
        "\n",
        "\n",
        "def extract_links_from_html(html: str) -> List[str]:\n",
        "    \"\"\"Parse Cars.com results page HTML and extract all vehicle detail links.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    tags = soup.select(\"a.vehicle-card-link\")\n",
        "    links: List[str] = []\n",
        "    for tag in tags:\n",
        "        href = tag.get(\"href\")\n",
        "        if href and \"/vehicledetail/\" in href:\n",
        "            links.append(urljoin(SITE_BASE, href))\n",
        "    return links\n",
        "\n",
        "\n",
        "def crawl_all_listing_urls(\n",
        "    start_page: int = 170,\n",
        "    end_page: int = 200,\n",
        "    output_dir: str = \"car_links\",\n",
        "    delay_between_pages: float = 2.0,\n",
        "    headless: bool = True,\n",
        "    driver_path: Optional[str] = None,\n",
        "    clear_output_first: bool = False,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Crawl Cars.com listings across multiple pages and store URLs page-by-page.\n",
        "\n",
        "    Each page will have its own file, e.g. car_links/page_1.txt, car_links/page_2.txt, etc.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if clear_output_first:\n",
        "        for file in os.listdir(output_dir):\n",
        "            if file.startswith(\"page_\") and file.endswith(\".txt\"):\n",
        "                os.remove(os.path.join(output_dir, file))\n",
        "\n",
        "    driver = init_driver(headless=headless, driver_path=driver_path)\n",
        "    try:\n",
        "        for page in range(start_page, end_page + 1):\n",
        "            url = RESULTS_URL_TMPL.format(page=page)\n",
        "            print(f\"Fetching page {page}: {url}\")\n",
        "\n",
        "            try:\n",
        "                driver.get(url)\n",
        "            except Exception as e:\n",
        "                print(f\"Error navigating to page {page}: {e}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                wait_for_cards(driver, timeout=20)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            scroll_to_bottom(driver, pause=0.6, max_rounds=12)\n",
        "            links = extract_links_from_html(driver.page_source)\n",
        "\n",
        "            if not links:\n",
        "                print(f\"No car links found on page {page}\")\n",
        "                continue\n",
        "\n",
        "            output_file = os.path.join(output_dir, f\"page_{page}.txt\")\n",
        "\n",
        "            with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "                for lnk in links:\n",
        "                    f_out.write(lnk + \"\\n\")\n",
        "\n",
        "            print(f\"Saved {len(links)} links to {output_file}\")\n",
        "\n",
        "            time.sleep(delay_between_pages + random.uniform(0.3, 1.2))\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "        print(f\"Done. All links saved to folder '{output_dir}'.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_page = 170\n",
        "    end_page = 200\n",
        "    crawl_all_listing_urls(\n",
        "        start_page=start_page,\n",
        "        end_page=end_page,\n",
        "        output_dir=\"car_links\",\n",
        "        delay_between_pages=2.0,\n",
        "        headless=True,\n",
        "        driver_path=None,  # or specify \"/usr/lib/chromium-browser/chromedriver\"\n",
        "        clear_output_first=False,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcLC7JB8yBzv",
        "outputId": "2074327c-fe5d-4b77-cf69-a2202f3a6a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Page 1: 22 URLs ===\n",
            "\n",
            "[STEP 1] Collecting listing HTML...\n",
            "[1/22] Collecting HTML: https://www.cars.com/vehicledetail/d285c5a6-db6b-440e-b139-ef9c3937db8c/?attribution_type=isa\n",
            "Navigating to https://www.cars.com/vehicledetail/d285c5a6-db6b-440e-b139-ef9c3937db8c/?attribution_type=isa...\n",
            "Page loaded in 13.16 seconds.\n",
            "  Saved -> html_cache/page_1/listings/1_8120663023.html\n",
            "[2/22] Collecting HTML: https://www.cars.com/vehicledetail/dfb2feae-5988-4a48-940e-0d53b19dacec/?attribution_type=isa\n",
            "Navigating to https://www.cars.com/vehicledetail/dfb2feae-5988-4a48-940e-0d53b19dacec/?attribution_type=isa...\n",
            "Page loaded in 6.96 seconds.\n",
            "  Saved -> html_cache/page_1/listings/2_7300785386.html\n",
            "[3/22] Collecting HTML: https://www.cars.com/vehicledetail/8ef3078a-4a46-4e2c-84fd-c0cc0d5226f4/\n",
            "Navigating to https://www.cars.com/vehicledetail/8ef3078a-4a46-4e2c-84fd-c0cc0d5226f4/...\n",
            "Page loaded in 5.70 seconds.\n",
            "  Saved -> html_cache/page_1/listings/3_8730294452.html\n",
            "[4/22] Collecting HTML: https://www.cars.com/vehicledetail/ec00f31d-d447-4d8f-8016-53933a18525d/\n",
            "Navigating to https://www.cars.com/vehicledetail/ec00f31d-d447-4d8f-8016-53933a18525d/...\n",
            "Page loaded in 4.95 seconds.\n",
            "  Saved -> html_cache/page_1/listings/4_8124637460.html\n",
            "[5/22] Collecting HTML: https://www.cars.com/vehicledetail/0bbb10aa-6f34-47b6-9018-76a28cf1c161/\n",
            "Navigating to https://www.cars.com/vehicledetail/0bbb10aa-6f34-47b6-9018-76a28cf1c161/...\n",
            "Page loaded in 9.76 seconds.\n",
            "  Saved -> html_cache/page_1/listings/5_5479006505.html\n",
            "[6/22] Collecting HTML: https://www.cars.com/vehicledetail/e9c99bc7-32e4-469a-a9db-5b923f58db3e/\n",
            "Navigating to https://www.cars.com/vehicledetail/e9c99bc7-32e4-469a-a9db-5b923f58db3e/...\n",
            "Page loaded in 5.66 seconds.\n",
            "  Saved -> html_cache/page_1/listings/6_2154873749.html\n",
            "[7/22] Collecting HTML: https://www.cars.com/vehicledetail/f664bc1d-2294-4f0a-a4c9-e562152097db/\n",
            "Navigating to https://www.cars.com/vehicledetail/f664bc1d-2294-4f0a-a4c9-e562152097db/...\n",
            "Page loaded in 8.53 seconds.\n",
            "  Saved -> html_cache/page_1/listings/7_6208028787.html\n",
            "[8/22] Collecting HTML: https://www.cars.com/vehicledetail/d9fc3fc0-08fe-47d4-9f79-06e40a75fe50/\n",
            "Navigating to https://www.cars.com/vehicledetail/d9fc3fc0-08fe-47d4-9f79-06e40a75fe50/...\n",
            "Page loaded in 6.19 seconds.\n",
            "  Saved -> html_cache/page_1/listings/8_3343605546.html\n",
            "[9/22] Collecting HTML: https://www.cars.com/vehicledetail/83feeefd-f27b-4d09-9eff-0913be985193/\n",
            "Navigating to https://www.cars.com/vehicledetail/83feeefd-f27b-4d09-9eff-0913be985193/...\n",
            "Page loaded in 8.49 seconds.\n",
            "  Saved -> html_cache/page_1/listings/9_9034858612.html\n",
            "[10/22] Collecting HTML: https://www.cars.com/vehicledetail/4d5bf8a2-eaef-45e9-b7fd-d162dbe5e021/\n",
            "Navigating to https://www.cars.com/vehicledetail/4d5bf8a2-eaef-45e9-b7fd-d162dbe5e021/...\n",
            "Page loaded in 6.12 seconds.\n",
            "  Saved -> html_cache/page_1/listings/10_9083797743.html\n",
            "[11/22] Collecting HTML: https://www.cars.com/vehicledetail/c629ebeb-7832-41e6-ab9a-fc10ab6c043d/\n",
            "Navigating to https://www.cars.com/vehicledetail/c629ebeb-7832-41e6-ab9a-fc10ab6c043d/...\n",
            "Page loaded in 7.67 seconds.\n",
            "  Saved -> html_cache/page_1/listings/11_3728690375.html\n",
            "[12/22] Collecting HTML: https://www.cars.com/vehicledetail/2ba18a99-cd69-41e0-954f-7a1e90f94cff/\n",
            "Navigating to https://www.cars.com/vehicledetail/2ba18a99-cd69-41e0-954f-7a1e90f94cff/...\n",
            "Page loaded in 6.63 seconds.\n",
            "  Saved -> html_cache/page_1/listings/12_8536835703.html\n",
            "[13/22] Collecting HTML: https://www.cars.com/vehicledetail/dae4e8bb-33cc-4b1e-9138-12de18105a70/\n",
            "Navigating to https://www.cars.com/vehicledetail/dae4e8bb-33cc-4b1e-9138-12de18105a70/...\n",
            "Page loaded in 7.45 seconds.\n",
            "  Saved -> html_cache/page_1/listings/13_7897069821.html\n",
            "[14/22] Collecting HTML: https://www.cars.com/vehicledetail/1b0199b6-e28b-4341-a946-adfb6539584f/\n",
            "Navigating to https://www.cars.com/vehicledetail/1b0199b6-e28b-4341-a946-adfb6539584f/...\n",
            "Page loaded in 5.74 seconds.\n",
            "  Saved -> html_cache/page_1/listings/14_6962131365.html\n",
            "[15/22] Collecting HTML: https://www.cars.com/vehicledetail/f5bf2d08-531f-4f26-8f76-565685d22c37/\n",
            "Navigating to https://www.cars.com/vehicledetail/f5bf2d08-531f-4f26-8f76-565685d22c37/...\n",
            "Page loaded in 6.62 seconds.\n",
            "  Saved -> html_cache/page_1/listings/15_4982283613.html\n",
            "[16/22] Collecting HTML: https://www.cars.com/vehicledetail/27d91ba6-3fcd-41a8-a54c-7a672f3f589b/\n",
            "Navigating to https://www.cars.com/vehicledetail/27d91ba6-3fcd-41a8-a54c-7a672f3f589b/...\n",
            "Page loaded in 8.47 seconds.\n",
            "  Saved -> html_cache/page_1/listings/16_1739812374.html\n",
            "[17/22] Collecting HTML: https://www.cars.com/vehicledetail/a3ec2b88-b24a-44d9-9d7a-5d1dfa1fc668/\n",
            "Navigating to https://www.cars.com/vehicledetail/a3ec2b88-b24a-44d9-9d7a-5d1dfa1fc668/...\n",
            "Page loaded in 5.64 seconds.\n",
            "  Saved -> html_cache/page_1/listings/17_6240626703.html\n",
            "[18/22] Collecting HTML: https://www.cars.com/vehicledetail/90b2fc3c-fe9e-441e-ae68-9b484ed77c62/\n",
            "Navigating to https://www.cars.com/vehicledetail/90b2fc3c-fe9e-441e-ae68-9b484ed77c62/...\n",
            "Page loaded in 8.44 seconds.\n",
            "  Saved -> html_cache/page_1/listings/18_6611194302.html\n",
            "[19/22] Collecting HTML: https://www.cars.com/vehicledetail/05bad995-1e32-412a-ab68-ec8ef07ea960/\n",
            "Navigating to https://www.cars.com/vehicledetail/05bad995-1e32-412a-ab68-ec8ef07ea960/...\n",
            "Page loaded in 5.40 seconds.\n",
            "  Saved -> html_cache/page_1/listings/19_2880948110.html\n",
            "[20/22] Collecting HTML: https://www.cars.com/vehicledetail/f481fa49-1d0a-4f9f-a715-0f4a7201f4fa/\n",
            "Navigating to https://www.cars.com/vehicledetail/f481fa49-1d0a-4f9f-a715-0f4a7201f4fa/...\n",
            "Page loaded in 9.12 seconds.\n",
            "  Saved -> html_cache/page_1/listings/20_7923798110.html\n",
            "[21/22] Collecting HTML: https://www.cars.com/vehicledetail/475258ab-bf39-43d6-a293-8fa01c37de8a/\n",
            "Navigating to https://www.cars.com/vehicledetail/475258ab-bf39-43d6-a293-8fa01c37de8a/...\n",
            "Page loaded in 5.97 seconds.\n",
            "  Saved -> html_cache/page_1/listings/21_2306176067.html\n",
            "[22/22] Collecting HTML: https://www.cars.com/vehicledetail/9efc8279-b1e6-43c2-a065-eb73d2e748b9/\n",
            "Navigating to https://www.cars.com/vehicledetail/9efc8279-b1e6-43c2-a065-eb73d2e748b9/...\n",
            "Page loaded in 5.86 seconds.\n",
            "  Saved -> html_cache/page_1/listings/22_5341558978.html\n",
            "Closing browser...\n",
            "Collected 22 listing HTML files\n",
            "\n",
            "[STEP 2] Parsing listings and collecting secondary URLs (using 5 threads)...\n",
            "Parsed 22 listings\n",
            "Found 22 seller links, 20 review links\n",
            "\n",
            "[STEP 3] Collecting seller HTML...\n",
            "[1/22] Collecting HTML: https://www.cars.com/dealers/5253854/seelye-kia-of-kalamazoo/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5253854/seelye-kia-of-kalamazoo/#Reviews...\n",
            "Page loaded in 7.63 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/1_5612064587.html\n",
            "[2/22] Collecting HTML: https://www.cars.com/dealers/149966/mountain-view-chevrolet/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/149966/mountain-view-chevrolet/#Reviews...\n",
            "Page loaded in 4.91 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/2_5198352719.html\n",
            "[3/22] Collecting HTML: https://www.cars.com/dealers/6062832/biggers-mitsubishi/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/6062832/biggers-mitsubishi/#Reviews...\n",
            "Page loaded in 2.67 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/3_6143395277.html\n",
            "[4/22] Collecting HTML: https://www.cars.com/dealers/336/autonation-toyota-libertyville/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/336/autonation-toyota-libertyville/#Reviews...\n",
            "Page loaded in 2.37 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/4_4094981008.html\n",
            "[5/22] Collecting HTML: https://www.cars.com/dealers/153335/krieger-ford/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/153335/krieger-ford/#Reviews...\n",
            "Page loaded in 4.31 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/5_9165248165.html\n",
            "[6/22] Collecting HTML: https://www.cars.com/dealers/109973/courtesy-palm-harbor-honda/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/109973/courtesy-palm-harbor-honda/#Reviews...\n",
            "Page loaded in 2.37 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/6_3130827750.html\n",
            "[7/22] Collecting HTML: https://www.cars.com/dealers/5382630/williamson-cadillac/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5382630/williamson-cadillac/#Reviews...\n",
            "Page loaded in 5.62 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/7_8699114277.html\n",
            "[8/22] Collecting HTML: https://www.cars.com/dealers/2543555/corwin-honda/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/2543555/corwin-honda/#Reviews...\n",
            "Page loaded in 4.41 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/8_6049176484.html\n",
            "[9/22] Collecting HTML: https://www.cars.com/dealers/5357599/classic-honda-of-midland/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5357599/classic-honda-of-midland/#Reviews...\n",
            "Page loaded in 2.49 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/9_6217739490.html\n",
            "[10/22] Collecting HTML: https://www.cars.com/dealers/23453/peterson-auto-group/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/23453/peterson-auto-group/#Reviews...\n",
            "Page loaded in 3.03 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/10_3182575821.html\n",
            "[11/22] Collecting HTML: https://www.cars.com/dealers/150057/cleveland-ford/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/150057/cleveland-ford/#Reviews...\n",
            "Page loaded in 4.30 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/11_5035182266.html\n",
            "[12/22] Collecting HTML: https://www.cars.com/dealers/210877/utter-family-of-dealerships/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/210877/utter-family-of-dealerships/#Reviews...\n",
            "Page loaded in 3.53 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/12_8596794840.html\n",
            "[13/22] Collecting HTML: https://www.cars.com/dealers/195343/bayird-dodge-chrysler-jeep-ram/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/195343/bayird-dodge-chrysler-jeep-ram/#Reviews...\n",
            "Page loaded in 2.42 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/13_6051973221.html\n",
            "[14/22] Collecting HTML: https://www.cars.com/dealers/5392880/freedom-chrysler-dodge-jeep-ram-north-by-ed-morse/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5392880/freedom-chrysler-dodge-jeep-ram-north-by-ed-morse/#Reviews...\n",
            "Page loaded in 4.99 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/14_2835910052.html\n",
            "[15/22] Collecting HTML: https://www.cars.com/dealers/6066709/alm-nissan-newnan/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/6066709/alm-nissan-newnan/#Reviews...\n",
            "Page loaded in 3.96 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/15_3235238743.html\n",
            "[16/22] Collecting HTML: https://www.cars.com/dealers/14094/liberty-ford-vermilion/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/14094/liberty-ford-vermilion/#Reviews...\n",
            "Page loaded in 2.76 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/16_5435435190.html\n",
            "[17/22] Collecting HTML: https://www.cars.com/dealers/25151/crown-dodge-chrysler-jeep-ram-ca/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/25151/crown-dodge-chrysler-jeep-ram-ca/#Reviews...\n",
            "Page loaded in 2.42 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/17_7635566999.html\n",
            "[18/22] Collecting HTML: https://www.cars.com/dealers/5327104/honolulu-gmc-cadillac/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5327104/honolulu-gmc-cadillac/#Reviews...\n",
            "Page loaded in 4.27 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/18_2157739042.html\n",
            "[19/22] Collecting HTML: https://www.cars.com/dealers/84912/ed-morse-sawgrass-auto-mall/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/84912/ed-morse-sawgrass-auto-mall/#Reviews...\n",
            "Page loaded in 2.49 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/19_6541956116.html\n",
            "[20/22] Collecting HTML: https://www.cars.com/dealers/104579/bournes-auto-center/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/104579/bournes-auto-center/#Reviews...\n",
            "Page loaded in 2.98 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/20_7606173631.html\n",
            "[21/22] Collecting HTML: https://www.cars.com/dealers/11140/mercedes-benz-of-columbus/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/11140/mercedes-benz-of-columbus/#Reviews...\n",
            "Page loaded in 6.30 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/21_4318134107.html\n",
            "[22/22] Collecting HTML: https://www.cars.com/dealers/197898/earnhardt-hyundai-north-scottsdale/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/197898/earnhardt-hyundai-north-scottsdale/#Reviews...\n",
            "Page loaded in 4.11 seconds.\n",
            "  Saved -> html_cache/page_1/sellers/22_2735118992.html\n",
            "Closing browser...\n",
            "Collected 22 seller HTML files\n",
            "\n",
            "[STEP 4] Collecting review HTML...\n",
            "[1/20] Collecting HTML: https://www.cars.com/research/mini-countryman-2020/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/mini-countryman-2020/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.42 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/1_1756142187.html\n",
            "[2/20] Collecting HTML: https://www.cars.com/research/nissan-altima-2019/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/nissan-altima-2019/consumer-reviews/?page_size=200...\n",
            "Page loaded in 7.00 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/2_1993922141.html\n",
            "[3/20] Collecting HTML: https://www.cars.com/research/mazda-cx_5-2023/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/mazda-cx_5-2023/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.74 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/3_7607168545.html\n",
            "[4/20] Collecting HTML: https://www.cars.com/research/gmc-sierra_1500-2025/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/gmc-sierra_1500-2025/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.19 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/4_8474576975.html\n",
            "[5/20] Collecting HTML: https://www.cars.com/research/chrysler-pacifica-2019/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/chrysler-pacifica-2019/consumer-reviews/?page_size=200...\n",
            "Page loaded in 6.50 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/5_1722976796.html\n",
            "[6/20] Collecting HTML: https://www.cars.com/research/cadillac-xt5-2022/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/cadillac-xt5-2022/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.48 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/6_1465322064.html\n",
            "[7/20] Collecting HTML: https://www.cars.com/research/infiniti-qx80-2019/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/infiniti-qx80-2019/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.84 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/7_1951062271.html\n",
            "[8/20] Collecting HTML: https://www.cars.com/research/nissan-altima-2021/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/nissan-altima-2021/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.40 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/8_9179814737.html\n",
            "[9/20] Collecting HTML: https://www.cars.com/research/buick-envision-2023/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/buick-envision-2023/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.13 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/9_8273468060.html\n",
            "[10/20] Collecting HTML: https://www.cars.com/research/porsche-cayenne-2019/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/porsche-cayenne-2019/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.93 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/10_7920470282.html\n",
            "[11/20] Collecting HTML: https://www.cars.com/research/lincoln-aviator-2021/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/lincoln-aviator-2021/consumer-reviews/?page_size=200...\n",
            "Page loaded in 5.42 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/11_3379032665.html\n",
            "[12/20] Collecting HTML: https://www.cars.com/research/jeep-grand_cherokee-2025/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/jeep-grand_cherokee-2025/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.10 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/12_2461052176.html\n",
            "[13/20] Collecting HTML: https://www.cars.com/research/kia-sorento-2024/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/kia-sorento-2024/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.55 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/13_8315482522.html\n",
            "[14/20] Collecting HTML: https://www.cars.com/research/jeep-wrangler-2024/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/jeep-wrangler-2024/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.63 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/14_6670254903.html\n",
            "[15/20] Collecting HTML: https://www.cars.com/research/honda-civic-2022/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/honda-civic-2022/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.45 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/15_4438128077.html\n",
            "[16/20] Collecting HTML: https://www.cars.com/research/scion-tc-2008/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/scion-tc-2008/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.13 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/16_4644124694.html\n",
            "[17/20] Collecting HTML: https://www.cars.com/research/buick-envista-2025/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/buick-envista-2025/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.83 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/17_4562289905.html\n",
            "[18/20] Collecting HTML: https://www.cars.com/research/polestar-2-2021/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/polestar-2-2021/consumer-reviews/?page_size=200...\n",
            "Page loaded in 5.43 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/18_8276716170.html\n",
            "[19/20] Collecting HTML: https://www.cars.com/research/volvo-xc90-2021/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/volvo-xc90-2021/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.47 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/19_6234733485.html\n",
            "[20/20] Collecting HTML: https://www.cars.com/research/volvo-xc60-2018/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/volvo-xc60-2018/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.63 seconds.\n",
            "  Saved -> html_cache/page_1/reviews/20_8611848931.html\n",
            "Closing browser...\n",
            "Collected 20 review HTML files\n",
            "\n",
            "[STEP 5] Parsing and saving final data (using 5 threads)...\n",
            "  [1] Complete -> raw_data/1/1.json\n",
            "  [2] Complete -> raw_data/1/2.json\n",
            "  [3] Partial (no reviews) -> raw_data/1/3.json\n",
            "  [4] Complete -> raw_data/1/4.json\n",
            "  [5] Complete -> raw_data/1/5.json\n",
            "  [6] Complete -> raw_data/1/6.json\n",
            "  [7] Complete -> raw_data/1/7.json\n",
            "  [8] Complete -> raw_data/1/8.json\n",
            "  [9] Complete -> raw_data/1/9.json\n",
            "  [10] Complete -> raw_data/1/10.json\n",
            "  [11] Complete -> raw_data/1/11.json\n",
            "  [12] Complete -> raw_data/1/12.json\n",
            "  [13] Complete -> raw_data/1/13.json\n",
            "  [14] Complete -> raw_data/1/14.json\n",
            "  [15] Complete -> raw_data/1/15.json\n",
            "  [16] Complete -> raw_data/1/16.json\n",
            "  [17] Complete -> raw_data/1/17.json\n",
            "  [18] Complete -> raw_data/1/18.json\n",
            "  [19] Partial (no reviews) -> raw_data/1/19.json\n",
            "  [20] Complete -> raw_data/1/20.json\n",
            "  [21] Complete -> raw_data/1/21.json\n",
            "  [22] Complete -> raw_data/1/22.json\n",
            "\n",
            "=== Page 2: 22 URLs ===\n",
            "\n",
            "[STEP 1] Collecting listing HTML...\n",
            "[1/22] Collecting HTML: https://www.cars.com/vehicledetail/bd0189db-db9a-4186-b5ba-ceee4917e693/?attribution_type=isa\n",
            "Navigating to https://www.cars.com/vehicledetail/bd0189db-db9a-4186-b5ba-ceee4917e693/?attribution_type=isa...\n",
            "Page loaded in 14.25 seconds.\n",
            "  Saved -> html_cache/page_2/listings/1_3640010491.html\n",
            "[2/22] Collecting HTML: https://www.cars.com/vehicledetail/0113ad08-a191-453b-b296-b4b437b1da9e/?attribution_type=isa\n",
            "Navigating to https://www.cars.com/vehicledetail/0113ad08-a191-453b-b296-b4b437b1da9e/?attribution_type=isa...\n",
            "Page loaded in 6.25 seconds.\n",
            "  Saved -> html_cache/page_2/listings/2_6748087851.html\n",
            "[3/22] Collecting HTML: https://www.cars.com/vehicledetail/ec8f0909-0ec5-4e80-8acc-c2695aadf024/\n",
            "Navigating to https://www.cars.com/vehicledetail/ec8f0909-0ec5-4e80-8acc-c2695aadf024/...\n",
            "Page loaded in 8.72 seconds.\n",
            "  Saved -> html_cache/page_2/listings/3_2232119314.html\n",
            "[4/22] Collecting HTML: https://www.cars.com/vehicledetail/da7f10cb-bcb3-40d4-921e-3030e7b82e46/\n",
            "Navigating to https://www.cars.com/vehicledetail/da7f10cb-bcb3-40d4-921e-3030e7b82e46/...\n",
            "Page loaded in 5.50 seconds.\n",
            "  Saved -> html_cache/page_2/listings/4_1295342218.html\n",
            "[5/22] Collecting HTML: https://www.cars.com/vehicledetail/faefef90-ae48-4108-b850-3dfc1c99cec5/\n",
            "Navigating to https://www.cars.com/vehicledetail/faefef90-ae48-4108-b850-3dfc1c99cec5/...\n",
            "Page loaded in 7.69 seconds.\n",
            "  Saved -> html_cache/page_2/listings/5_3264193991.html\n",
            "[6/22] Collecting HTML: https://www.cars.com/vehicledetail/5d841284-17ed-4cdb-bf49-75e7457327a3/\n",
            "Navigating to https://www.cars.com/vehicledetail/5d841284-17ed-4cdb-bf49-75e7457327a3/...\n",
            "Page loaded in 6.42 seconds.\n",
            "  Saved -> html_cache/page_2/listings/6_8740063929.html\n",
            "[7/22] Collecting HTML: https://www.cars.com/vehicledetail/ed8cec89-f4b2-484e-a281-ab7c767e00d3/\n",
            "Navigating to https://www.cars.com/vehicledetail/ed8cec89-f4b2-484e-a281-ab7c767e00d3/...\n",
            "Page loaded in 8.60 seconds.\n",
            "  Saved -> html_cache/page_2/listings/7_2914081617.html\n",
            "[8/22] Collecting HTML: https://www.cars.com/vehicledetail/7dca1448-7105-458f-8b53-507e778fc41f/\n",
            "Navigating to https://www.cars.com/vehicledetail/7dca1448-7105-458f-8b53-507e778fc41f/...\n",
            "Page loaded in 8.41 seconds.\n",
            "  Saved -> html_cache/page_2/listings/8_4704657951.html\n",
            "[9/22] Collecting HTML: https://www.cars.com/vehicledetail/eb13a32c-a84b-4abd-bb41-14526502dec9/\n",
            "Navigating to https://www.cars.com/vehicledetail/eb13a32c-a84b-4abd-bb41-14526502dec9/...\n",
            "Page loaded in 7.05 seconds.\n",
            "  Saved -> html_cache/page_2/listings/9_8529911829.html\n",
            "[10/22] Collecting HTML: https://www.cars.com/vehicledetail/71c92ecf-232e-44c2-8c4d-9ed4d02f7015/\n",
            "Navigating to https://www.cars.com/vehicledetail/71c92ecf-232e-44c2-8c4d-9ed4d02f7015/...\n",
            "Page loaded in 8.80 seconds.\n",
            "  Saved -> html_cache/page_2/listings/10_3114961672.html\n",
            "[11/22] Collecting HTML: https://www.cars.com/vehicledetail/121d1ed2-3ad5-4d2f-8af6-0d4c880765d2/\n",
            "Navigating to https://www.cars.com/vehicledetail/121d1ed2-3ad5-4d2f-8af6-0d4c880765d2/...\n",
            "Page loaded in 10.93 seconds.\n",
            "  Saved -> html_cache/page_2/listings/11_9021392639.html\n",
            "[12/22] Collecting HTML: https://www.cars.com/vehicledetail/d44d6f31-8908-4765-82ec-99c6ba296e14/\n",
            "Navigating to https://www.cars.com/vehicledetail/d44d6f31-8908-4765-82ec-99c6ba296e14/...\n",
            "Page loaded in 6.73 seconds.\n",
            "  Saved -> html_cache/page_2/listings/12_8146339498.html\n",
            "[13/22] Collecting HTML: https://www.cars.com/vehicledetail/c632dbc0-4c9c-4ad9-ac2e-7f7aebc18dfe/\n",
            "Navigating to https://www.cars.com/vehicledetail/c632dbc0-4c9c-4ad9-ac2e-7f7aebc18dfe/...\n",
            "Page loaded in 7.10 seconds.\n",
            "  Saved -> html_cache/page_2/listings/13_1963365695.html\n",
            "[14/22] Collecting HTML: https://www.cars.com/vehicledetail/e9ede950-21aa-4182-8d14-c4ab0460892b/\n",
            "Navigating to https://www.cars.com/vehicledetail/e9ede950-21aa-4182-8d14-c4ab0460892b/...\n",
            "Page loaded in 7.53 seconds.\n",
            "  Saved -> html_cache/page_2/listings/14_5890200802.html\n",
            "[15/22] Collecting HTML: https://www.cars.com/vehicledetail/4d7ce141-48ad-4c31-b523-80e851831a0b/\n",
            "Navigating to https://www.cars.com/vehicledetail/4d7ce141-48ad-4c31-b523-80e851831a0b/...\n",
            "Page loaded in 10.65 seconds.\n",
            "  Saved -> html_cache/page_2/listings/15_5056994099.html\n",
            "[16/22] Collecting HTML: https://www.cars.com/vehicledetail/fa90c752-1604-440a-ad5a-0390c07170c0/\n",
            "Navigating to https://www.cars.com/vehicledetail/fa90c752-1604-440a-ad5a-0390c07170c0/...\n",
            "Page loaded in 5.54 seconds.\n",
            "  Saved -> html_cache/page_2/listings/16_4453612981.html\n",
            "[17/22] Collecting HTML: https://www.cars.com/vehicledetail/1f31cfa1-1199-4db0-aa9c-6a4f7fa6dc44/\n",
            "Navigating to https://www.cars.com/vehicledetail/1f31cfa1-1199-4db0-aa9c-6a4f7fa6dc44/...\n",
            "Page loaded in 5.71 seconds.\n",
            "  Saved -> html_cache/page_2/listings/17_1792987403.html\n",
            "[18/22] Collecting HTML: https://www.cars.com/vehicledetail/8854e1ad-c974-47f0-a1b2-a6c105009414/\n",
            "Navigating to https://www.cars.com/vehicledetail/8854e1ad-c974-47f0-a1b2-a6c105009414/...\n",
            "Page loaded in 7.01 seconds.\n",
            "  Saved -> html_cache/page_2/listings/18_1698804024.html\n",
            "[19/22] Collecting HTML: https://www.cars.com/vehicledetail/df528b31-73f9-4bdf-85d6-32c228d0e0ce/\n",
            "Navigating to https://www.cars.com/vehicledetail/df528b31-73f9-4bdf-85d6-32c228d0e0ce/...\n",
            "Page loaded in 8.91 seconds.\n",
            "  Saved -> html_cache/page_2/listings/19_4714325104.html\n",
            "[20/22] Collecting HTML: https://www.cars.com/vehicledetail/6e35c3b9-662e-48df-9fcc-24ebc4a7ae70/\n",
            "Navigating to https://www.cars.com/vehicledetail/6e35c3b9-662e-48df-9fcc-24ebc4a7ae70/...\n",
            "Page loaded in 6.27 seconds.\n",
            "  Saved -> html_cache/page_2/listings/20_1486250420.html\n",
            "[21/22] Collecting HTML: https://www.cars.com/vehicledetail/a5fb93c2-88ba-4839-b1ca-d44a63f680f5/\n",
            "Navigating to https://www.cars.com/vehicledetail/a5fb93c2-88ba-4839-b1ca-d44a63f680f5/...\n",
            "Page loaded in 9.46 seconds.\n",
            "  Saved -> html_cache/page_2/listings/21_7544458182.html\n",
            "[22/22] Collecting HTML: https://www.cars.com/vehicledetail/6d0df3da-ba99-4425-a68a-0bca06ca205c/\n",
            "Navigating to https://www.cars.com/vehicledetail/6d0df3da-ba99-4425-a68a-0bca06ca205c/...\n",
            "Page loaded in 10.28 seconds.\n",
            "  Saved -> html_cache/page_2/listings/22_6222403268.html\n",
            "Closing browser...\n",
            "Collected 22 listing HTML files\n",
            "\n",
            "[STEP 2] Parsing listings and collecting secondary URLs (using 5 threads)...\n",
            "Parsed 22 listings\n",
            "Found 22 seller links, 20 review links\n",
            "\n",
            "[STEP 3] Collecting seller HTML...\n",
            "[1/22] Collecting HTML: https://www.cars.com/dealers/5362884/florida-fine-cars-wpb/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5362884/florida-fine-cars-wpb/#Reviews...\n",
            "Page loaded in 6.12 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/1_5423434918.html\n",
            "[2/22] Collecting HTML: https://www.cars.com/dealers/6062832/biggers-mitsubishi/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/6062832/biggers-mitsubishi/#Reviews...\n",
            "Page loaded in 3.95 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/2_6143395277.html\n",
            "[3/22] Collecting HTML: https://www.cars.com/dealers/514/the-chevrolet-exchange/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/514/the-chevrolet-exchange/#Reviews...\n",
            "Page loaded in 2.51 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/3_5221839041.html\n",
            "[4/22] Collecting HTML: https://www.cars.com/dealers/5342304/kirksville-motor-company/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5342304/kirksville-motor-company/#Reviews...\n",
            "Page loaded in 4.65 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/4_5837943596.html\n",
            "[5/22] Collecting HTML: https://www.cars.com/dealers/18941/zimmerman-honda/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/18941/zimmerman-honda/#Reviews...\n",
            "Page loaded in 4.66 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/5_5395734737.html\n",
            "[6/22] Collecting HTML: https://www.cars.com/dealers/11509/orlando-infinitiaston-martin-orlandolotus-orlandomoke-america-orlando/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/11509/orlando-infinitiaston-martin-orlandolotus-orlandomoke-america-orlando/#Reviews...\n",
            "Page loaded in 2.43 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/6_5998049533.html\n",
            "[7/22] Collecting HTML: https://www.cars.com/dealers/6058169/canton-ford/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/6058169/canton-ford/#Reviews...\n",
            "Page loaded in 2.36 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/7_3643894302.html\n",
            "[8/22] Collecting HTML: https://www.cars.com/dealers/155649/dave-white-chevrolet/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/155649/dave-white-chevrolet/#Reviews...\n",
            "Page loaded in 4.17 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/8_7413455598.html\n",
            "[9/22] Collecting HTML: https://www.cars.com/dealers/5244466/south-pointe-honda/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5244466/south-pointe-honda/#Reviews...\n",
            "Page loaded in 2.29 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/9_4530911757.html\n",
            "[10/22] Collecting HTML: https://www.cars.com/dealers/97046/lone-star-chevrolet/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/97046/lone-star-chevrolet/#Reviews...\n",
            "Page loaded in 2.29 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/10_4956767238.html\n",
            "[11/22] Collecting HTML: https://www.cars.com/dealers/157478/hawkins-chevrolet/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/157478/hawkins-chevrolet/#Reviews...\n",
            "Page loaded in 6.61 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/11_6099510280.html\n",
            "[12/22] Collecting HTML: https://www.cars.com/dealers/4636/bmw-of-ramsey/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/4636/bmw-of-ramsey/#Reviews...\n",
            "Page loaded in 4.26 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/12_4131037354.html\n",
            "[13/22] Collecting HTML: https://www.cars.com/dealers/152472/laird-noller-ford-topeka/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/152472/laird-noller-ford-topeka/#Reviews...\n",
            "Page loaded in 2.44 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/13_6575041450.html\n",
            "[14/22] Collecting HTML: https://www.cars.com/dealers/5243533/kia-on-atlantic/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5243533/kia-on-atlantic/#Reviews...\n",
            "Page loaded in 2.33 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/14_1037652417.html\n",
            "[15/22] Collecting HTML: https://www.cars.com/dealers/24067/fletcher-jones-imports/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/24067/fletcher-jones-imports/#Reviews...\n",
            "Page loaded in 3.79 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/15_1201169689.html\n",
            "[16/22] Collecting HTML: https://www.cars.com/dealers/90440/bmw-of-sarasota/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/90440/bmw-of-sarasota/#Reviews...\n",
            "Page loaded in 2.67 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/16_6132454817.html\n",
            "[17/22] Collecting HTML: https://www.cars.com/dealers/24881/toyota-of-the-desert/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/24881/toyota-of-the-desert/#Reviews...\n",
            "Page loaded in 2.40 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/17_2204486078.html\n",
            "[18/22] Collecting HTML: https://www.cars.com/dealers/19353/isringhausen-imports/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/19353/isringhausen-imports/#Reviews...\n",
            "Page loaded in 4.29 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/18_8824546440.html\n",
            "[19/22] Collecting HTML: https://www.cars.com/dealers/96082/findlay-toyota/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/96082/findlay-toyota/#Reviews...\n",
            "Page loaded in 4.22 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/19_8969550921.html\n",
            "[20/22] Collecting HTML: https://www.cars.com/dealers/158336/lithia-chrysler-jeep-dodge-ram-fiat-of-tri-cities/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/158336/lithia-chrysler-jeep-dodge-ram-fiat-of-tri-cities/#Reviews...\n",
            "Page loaded in 2.22 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/20_9169495052.html\n",
            "[21/22] Collecting HTML: https://www.cars.com/dealers/147506/davis-gainesville-chevrolet/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/147506/davis-gainesville-chevrolet/#Reviews...\n",
            "Page loaded in 2.74 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/21_7712242834.html\n",
            "[22/22] Collecting HTML: https://www.cars.com/dealers/5377645/honda-of-lake-city/#Reviews\n",
            "Navigating to https://www.cars.com/dealers/5377645/honda-of-lake-city/#Reviews...\n",
            "Page loaded in 3.29 seconds.\n",
            "  Saved -> html_cache/page_2/sellers/22_3008393393.html\n",
            "Closing browser...\n",
            "Collected 22 seller HTML files\n",
            "\n",
            "[STEP 4] Collecting review HTML...\n",
            "[1/20] Collecting HTML: https://www.cars.com/research/maserati-levante-2022/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/maserati-levante-2022/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.80 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/1_4731651761.html\n",
            "[2/20] Collecting HTML: https://www.cars.com/research/ram-1500-2022/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/ram-1500-2022/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.33 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/2_9048306194.html\n",
            "[3/20] Collecting HTML: https://www.cars.com/research/gmc-sierra_1500-2024/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/gmc-sierra_1500-2024/consumer-reviews/?page_size=200...\n",
            "Page loaded in 5.11 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/3_3154424617.html\n",
            "[4/20] Collecting HTML: https://www.cars.com/research/buick-enclave-2021/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/buick-enclave-2021/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.49 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/4_8843779570.html\n",
            "[5/20] Collecting HTML: https://www.cars.com/research/honda-passport-2026/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/honda-passport-2026/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.44 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/5_4999914642.html\n",
            "[6/20] Collecting HTML: https://www.cars.com/research/infiniti-qx80-2025/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/infiniti-qx80-2025/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.69 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/6_7948768426.html\n",
            "[7/20] Collecting HTML: https://www.cars.com/research/mazda-mazda3-2024/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/mazda-mazda3-2024/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.32 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/7_1313845576.html\n",
            "[8/20] Collecting HTML: https://www.cars.com/research/jeep-grand_cherokee-2022/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/jeep-grand_cherokee-2022/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.61 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/8_5002945636.html\n",
            "[9/20] Collecting HTML: https://www.cars.com/research/volkswagen-atlas-2021/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/volkswagen-atlas-2021/consumer-reviews/?page_size=200...\n",
            "Page loaded in 7.67 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/9_5382683017.html\n",
            "[10/20] Collecting HTML: https://www.cars.com/research/buick-envision-2023/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/buick-envision-2023/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.48 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/10_8273468060.html\n",
            "[11/20] Collecting HTML: https://www.cars.com/research/alfa_romeo-stelvio-2019/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/alfa_romeo-stelvio-2019/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.83 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/11_2167335921.html\n",
            "[12/20] Collecting HTML: https://www.cars.com/research/lincoln-aviator-2023/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/lincoln-aviator-2023/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.63 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/12_7877417181.html\n",
            "[13/20] Collecting HTML: https://www.cars.com/research/ram-1500-2022/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/ram-1500-2022/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.78 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/13_9048306194.html\n",
            "[14/20] Collecting HTML: https://www.cars.com/research/mercedes_benz-gls_450-2025/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/mercedes_benz-gls_450-2025/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.62 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/14_8194261423.html\n",
            "[15/20] Collecting HTML: https://www.cars.com/research/mini-hardtop-2020/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/mini-hardtop-2020/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.71 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/15_4680749250.html\n",
            "[16/20] Collecting HTML: https://www.cars.com/research/volkswagen-atlas-2024/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/volkswagen-atlas-2024/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.98 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/16_1592444806.html\n",
            "[17/20] Collecting HTML: https://www.cars.com/research/porsche-911-2011/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/porsche-911-2011/consumer-reviews/?page_size=200...\n",
            "Page loaded in 1.98 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/17_1721505375.html\n",
            "[18/20] Collecting HTML: https://www.cars.com/research/acura-mdx-2024/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/acura-mdx-2024/consumer-reviews/?page_size=200...\n",
            "Page loaded in 2.45 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/18_3074212586.html\n",
            "[19/20] Collecting HTML: https://www.cars.com/research/maserati-levante-2021/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/maserati-levante-2021/consumer-reviews/?page_size=200...\n",
            "Page loaded in 3.98 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/19_5780884965.html\n",
            "[20/20] Collecting HTML: https://www.cars.com/research/dodge-charger-2022/consumer-reviews/?page_size=200\n",
            "Navigating to https://www.cars.com/research/dodge-charger-2022/consumer-reviews/?page_size=200...\n",
            "Page loaded in 4.24 seconds.\n",
            "  Saved -> html_cache/page_2/reviews/20_5708722549.html\n",
            "Closing browser...\n",
            "Collected 19 review HTML files\n",
            "\n",
            "[STEP 5] Parsing and saving final data (using 5 threads)...\n",
            "  [1] Complete -> raw_data/2/1.json\n",
            "  [2] Complete -> raw_data/2/2.json\n",
            "  [3] Complete -> raw_data/2/3.json\n",
            "  [4] Complete -> raw_data/2/4.json\n",
            "  [5] Complete -> raw_data/2/5.json\n",
            "  [6] Complete -> raw_data/2/6.json\n",
            "  [7] Complete -> raw_data/2/7.json\n",
            "  [8] Complete -> raw_data/2/8.json\n",
            "  [9] Complete -> raw_data/2/9.json\n",
            "  [10] Partial (no reviews) -> raw_data/2/10.json\n",
            "  [11] Complete -> raw_data/2/11.json\n",
            "  [12] Complete -> raw_data/2/12.json\n",
            "  [13] Complete -> raw_data/2/13.json\n",
            "  [14] Complete -> raw_data/2/14.json\n",
            "  [15] Complete -> raw_data/2/15.json\n",
            "  [16] Complete -> raw_data/2/16.json\n",
            "  [17] Complete -> raw_data/2/17.json\n",
            "  [18] Complete -> raw_data/2/18.json\n",
            "  [19] Complete -> raw_data/2/19.json\n",
            "  [20] Complete -> raw_data/2/20.json\n",
            "  [21] Partial (no reviews) -> raw_data/2/21.json\n",
            "  [22] Complete -> raw_data/2/22.json\n",
            "\n",
            "=== Scraping Complete ===\n",
            "Pages processed: 1 to 2\n",
            "Output saved to: raw_data\n",
            "HTML cache saved to: html_cache\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# BATCH SCRAPING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def read_urls_from_file(file_path: str) -> List[str]:\n",
        "    \"\"\"Read all non-empty lines (URLs) from a file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"URL file not found: {file_path}\")\n",
        "        return []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "\n",
        "def categorize_scraping_result(data: Optional[Dict], url: str) -> str:\n",
        "    \"\"\"\n",
        "    Categorize scraping result based on data completeness.\n",
        "\n",
        "    Returns:\n",
        "        'complete': Full data with reviews\n",
        "        'partial_no_reviews': Complete listing but no reviews (new model)\n",
        "        'partial': Incomplete data\n",
        "        'failed': No data retrieved\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        return 'failed'\n",
        "\n",
        "    metadata = data.get('_metadata', {})\n",
        "\n",
        "    if metadata.get('is_complete'):\n",
        "        if metadata.get('has_car_link') and metadata.get('has_ratings'):\n",
        "            return 'complete'\n",
        "        else:\n",
        "            return 'partial_no_reviews'\n",
        "\n",
        "    return 'partial'\n",
        "\n",
        "\n",
        "def scrape_one_car(i, url, page_number, page_output_dir, headless, delay):\n",
        "    \"\"\"\n",
        "    Scrape one car listing and save JSON output with result categorization.\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(page_output_dir, f\"{i}.json\")\n",
        "\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"[Page {page_number}] Car {i}: Skipping (exists)\")\n",
        "        return\n",
        "\n",
        "    print(f\"[Page {page_number}] Car {i}: Scraping {url}\")\n",
        "    try:\n",
        "        data = scrape_full_info(url, headless=headless)\n",
        "\n",
        "        result_type = categorize_scraping_result(data, url)\n",
        "\n",
        "        if result_type == 'failed':\n",
        "            print(f\"  Failed - no data retrieved\")\n",
        "            return\n",
        "\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        if result_type == 'complete':\n",
        "            print(f\"  Complete -> {output_path}\")\n",
        "        elif result_type == 'partial_no_reviews':\n",
        "            print(f\"  Partial (no reviews) -> {output_path}\")\n",
        "        else:\n",
        "            print(f\"  Partial -> {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "\n",
        "    time.sleep(delay + random.uniform(0.5, 1.5))\n",
        "\n",
        "\n",
        "def scrape_from_url_files_batch(\n",
        "    link_folder: str = \"car_links\",\n",
        "    output_root: str = \"raw_data\",\n",
        "    html_cache_root: str = \"html_cache\",\n",
        "    headless: bool = True,\n",
        "    from_page: int = 170,\n",
        "    to_page: int = 200,\n",
        "    max_workers: int = 5,\n",
        "):\n",
        "    \"\"\"\n",
        "    IMPROVED: Batch HTML collection then parsing with multi-threading.\n",
        "    - Opens browser once per page\n",
        "    - Collects all HTML first\n",
        "    - Then parses offline with MULTI-THREADING (NEW!)\n",
        "    - Avoids OOM from frequent browser open/close\n",
        "\n",
        "    Args:\n",
        "        link_folder: Folder containing page_X.txt files with car URLs.\n",
        "        output_root: Root folder to save scraped JSON data.\n",
        "        html_cache_root: Root folder to cache HTML files.\n",
        "        headless: Whether to run browser in headless mode.\n",
        "        from_page: Starting page number.\n",
        "        to_page: Ending page number (inclusive).\n",
        "        max_workers: Number of parallel threads for parsing (default 10).\n",
        "    \"\"\"\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "    os.makedirs(html_cache_root, exist_ok=True)\n",
        "\n",
        "    for page_number in range(from_page, to_page + 1):\n",
        "        file_name = f\"page_{page_number}.txt\"\n",
        "        file_path = os.path.join(link_folder, file_name)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"File not found: {file_name} - skipping.\")\n",
        "            continue\n",
        "\n",
        "        urls = read_urls_from_file(file_path)\n",
        "        print(f\"\\n=== Page {page_number}: {len(urls)} URLs ===\")\n",
        "\n",
        "        # Step 1: Collect all listing HTML (single browser session)\n",
        "        print(f\"\\n[STEP 1] Collecting listing HTML...\")\n",
        "        html_dir = os.path.join(html_cache_root, f\"page_{page_number}\", \"listings\")\n",
        "        url_to_html = collect_html_batch(urls, html_dir, headless=headless)\n",
        "        print(f\"Collected {len(url_to_html)} listing HTML files\")\n",
        "\n",
        "        # Step 2: Parse listings and collect secondary URLs (MULTI-THREADED)\n",
        "        print(f\"\\n[STEP 2] Parsing listings and collecting secondary URLs (using {max_workers} threads)...\")\n",
        "        seller_urls = []\n",
        "        review_urls = []\n",
        "        listing_data_map = {}\n",
        "\n",
        "        def parse_single_listing(url_html_pair):\n",
        "            \"\"\"Helper function to parse a single listing in parallel.\"\"\"\n",
        "            url, html_path = url_html_pair\n",
        "            try:\n",
        "                html_content = load_html(html_path)\n",
        "                if not html_content:\n",
        "                    return None\n",
        "\n",
        "                data = scrape_post_from_html(html_content, url)\n",
        "                if data:\n",
        "                    seller_link = data.get('seller', {}).get('seller_link')\n",
        "                    review_link = data.get('car', {}).get('review_link')\n",
        "                    return (url, data, seller_link, review_link)\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Parse in parallel\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = [executor.submit(parse_single_listing, item) for item in url_to_html.items()]\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    url, data, seller_link, review_link = result\n",
        "                    listing_data_map[url] = data\n",
        "\n",
        "                    if seller_link:\n",
        "                        seller_urls.append((url, seller_link))\n",
        "\n",
        "                    if review_link:\n",
        "                        review_urls.append((url, review_link))\n",
        "\n",
        "        print(f\"Parsed {len(listing_data_map)} listings\")\n",
        "        print(f\"Found {len(seller_urls)} seller links, {len(review_urls)} review links\")\n",
        "\n",
        "        # Step 3: Collect seller HTML (single browser session)\n",
        "        if seller_urls:\n",
        "            print(f\"\\n[STEP 3] Collecting seller HTML...\")\n",
        "            seller_html_dir = os.path.join(html_cache_root, f\"page_{page_number}\", \"sellers\")\n",
        "            seller_only_urls = [s[1] for s in seller_urls]\n",
        "            seller_url_to_html = collect_html_batch(seller_only_urls, seller_html_dir, headless=headless)\n",
        "            print(f\"Collected {len(seller_url_to_html)} seller HTML files\")\n",
        "        else:\n",
        "            seller_url_to_html = {}\n",
        "\n",
        "        # Step 4: Collect review HTML (single browser session)\n",
        "        if review_urls:\n",
        "            print(f\"\\n[STEP 4] Collecting review HTML...\")\n",
        "            review_html_dir = os.path.join(html_cache_root, f\"page_{page_number}\", \"reviews\")\n",
        "            review_only_urls = [r[1] for r in review_urls]\n",
        "            review_url_to_html = collect_html_batch(review_only_urls, review_html_dir, headless=headless)\n",
        "            print(f\"Collected {len(review_url_to_html)} review HTML files\")\n",
        "        else:\n",
        "            review_url_to_html = {}\n",
        "\n",
        "        # Step 5: Parse all data and save JSON (MULTI-THREADED)\n",
        "        print(f\"\\n[STEP 5] Parsing and saving final data (using {max_workers} threads)...\")\n",
        "        page_output_dir = os.path.join(output_root, str(page_number))\n",
        "        os.makedirs(page_output_dir, exist_ok=True)\n",
        "\n",
        "        def process_final_data(url_idx_pair):\n",
        "            \"\"\"Helper function to process final data in parallel.\"\"\"\n",
        "            idx, url = url_idx_pair\n",
        "            try:\n",
        "                if url not in listing_data_map:\n",
        "                    return None\n",
        "\n",
        "                data = copy.deepcopy(listing_data_map[url])\n",
        "\n",
        "                # Get seller HTML if exists\n",
        "                seller_link = data.get('seller', {}).get('seller_link')\n",
        "                if seller_link and seller_link in seller_url_to_html:\n",
        "                    seller_html = load_html(seller_url_to_html[seller_link])\n",
        "                    if seller_html:\n",
        "                        data = scrape_seller_from_html(seller_html, data)\n",
        "\n",
        "                # Get review HTML if exists\n",
        "                review_link = data.get('car', {}).get('review_link')\n",
        "                if review_link and review_link in review_url_to_html:\n",
        "                    review_html = load_html(review_url_to_html[review_link])\n",
        "                    if review_html:\n",
        "                        data = scrape_review_from_html(review_html, data)\n",
        "\n",
        "                # Save JSON\n",
        "                output_path = os.path.join(page_output_dir, f\"{idx}.json\")\n",
        "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "                result_type = categorize_scraping_result(data, url)\n",
        "                return (idx, result_type, output_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  [{idx}] Error: {e}\")\n",
        "                return None\n",
        "\n",
        "        # Process in parallel\n",
        "        url_idx_pairs = [(idx, url) for idx, url in enumerate(urls, 1)]\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = [executor.submit(process_final_data, pair) for pair in url_idx_pairs]\n",
        "\n",
        "            results = []\n",
        "            for future in as_completed(futures):\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "            # Sort and print results in order\n",
        "            results.sort(key=lambda x: x[0])\n",
        "            for idx, result_type, output_path in results:\n",
        "                if result_type == 'complete':\n",
        "                    print(f\"  [{idx}] Complete -> {output_path}\")\n",
        "                elif result_type == 'partial_no_reviews':\n",
        "                    print(f\"  [{idx}] Partial (no reviews) -> {output_path}\")\n",
        "                else:\n",
        "                    print(f\"  [{idx}] Partial -> {output_path}\")\n",
        "\n",
        "    print(f\"\\n=== Scraping Complete ===\")\n",
        "    print(f\"Pages processed: {from_page} to {to_page}\")\n",
        "    print(f\"Output saved to: {output_root}\")\n",
        "    print(f\"HTML cache saved to: {html_cache_root}\")\n",
        "\n",
        "\n",
        "def scrape_from_url_files(\n",
        "    link_folder: str = \"car_links\",\n",
        "    output_root: str = \"raw_data\",\n",
        "    delay: float = 2.0,\n",
        "    headless: bool = True,\n",
        "    from_page: int = 170,\n",
        "    to_page: int = 200,\n",
        "    max_workers: int = 5,\n",
        "):\n",
        "    \"\"\"\n",
        "    DEPRECATED: Old multi-threaded approach with frequent browser open/close.\n",
        "    Use scrape_from_url_files_batch() instead for better performance.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "    for page_number in range(from_page, to_page + 1):\n",
        "        file_name = f\"page_{page_number}.txt\"\n",
        "        file_path = os.path.join(link_folder, file_name)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"File not found: {file_name} - skipping.\")\n",
        "            continue\n",
        "\n",
        "        page_output_dir = os.path.join(output_root, str(page_number))\n",
        "        os.makedirs(page_output_dir, exist_ok=True)\n",
        "\n",
        "        urls = read_urls_from_file(file_path)\n",
        "        print(f\"\\n=== Processing {file_name}: {len(urls)} URLs ===\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = [\n",
        "                executor.submit(scrape_one_car, i, url, page_number, page_output_dir, headless, delay)\n",
        "                for i, url in enumerate(urls, start=1)\n",
        "            ]\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    future.result()\n",
        "                except Exception as e:\n",
        "                    print(f\"  Thread error: {e}\")\n",
        "\n",
        "    print(f\"\\n=== Scraping Complete ===\")\n",
        "    print(f\"Pages processed: {from_page} to {to_page}\")\n",
        "    print(f\"Output saved to: {output_root}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # NEW IMPROVED METHOD: Batch HTML collection with multi-threading\n",
        "    scrape_from_url_files_batch(\n",
        "        link_folder=\"car_links\",\n",
        "        output_root=\"raw_data\",\n",
        "        html_cache_root=\"html_cache\",\n",
        "        headless=True,\n",
        "        from_page=170,\n",
        "        to_page=200,\n",
        "        max_workers=5,  # Number of parallel threads for parsing\n",
        "    )\n",
        "\n",
        "    # OLD METHOD (deprecated - causes OOM from frequent browser open/close)\n",
        "    # scrape_from_url_files(\n",
        "    #     link_folder=\"car_links\",\n",
        "    #     output_root=\"raw_data\",\n",
        "    #     delay=2.5,\n",
        "    #     headless=True,\n",
        "    #     from_page=1,\n",
        "    #     to_page=2,\n",
        "    #     max_workers=5,\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iSeE9Wu-4nm"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/raw_data/* \"/content/drive/MyDrive/Car Recsys Consultant Chatbot/\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
